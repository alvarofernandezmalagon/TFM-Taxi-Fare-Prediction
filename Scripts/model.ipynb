{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "alt.renderers.enable(\"notebook\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#from sklearn.utils import check_arrays\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_trips = pd.read_csv('../Data/taxi_model_sample/part-00000-297319ca-4b71-48dc-a986-8a239b563270-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>fare</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_centroid_latitude</th>\n",
       "      <th>pickup_centroid_longitude</th>\n",
       "      <th>dropoff_centroid_latitude</th>\n",
       "      <th>dropoff_centroid_longitude</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>week_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>distance_miles</th>\n",
       "      <th>distance_ord</th>\n",
       "      <th>distance_mdw</th>\n",
       "      <th>taxi_id_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52461</th>\n",
       "      <td>40711a363473d44fdfeed72464024e35c46cd180</td>\n",
       "      <td>240</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Dispatch Taxi Affiliation</td>\n",
       "      <td>41.884987</td>\n",
       "      <td>-87.620993</td>\n",
       "      <td>41.892508</td>\n",
       "      <td>-87.626215</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>275.480000</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>290.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632597</td>\n",
       "      <td>20.161236</td>\n",
       "      <td>11.373879</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20175</th>\n",
       "      <td>83b144b4dd2674bb052e9a8bafb9ab5198897392</td>\n",
       "      <td>1380</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "      <td>16.65</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.794090</td>\n",
       "      <td>-87.592311</td>\n",
       "      <td>41.740206</td>\n",
       "      <td>-87.615970</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>284.959000</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>209.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.064949</td>\n",
       "      <td>25.086632</td>\n",
       "      <td>9.961739</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30072</th>\n",
       "      <td>1992605e75f6a48a08fe96788d6751a025375e4d</td>\n",
       "      <td>420</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6.45</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Dispatch Taxi Affiliation</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>285.390333</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>208.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.550617</td>\n",
       "      <td>11.375130</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50956</th>\n",
       "      <td>33521c729d3c319dbd7d3cc7666210c46c39df59</td>\n",
       "      <td>900</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10.75</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.929263</td>\n",
       "      <td>-87.635891</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>281.893000</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2.585329</td>\n",
       "      <td>18.904905</td>\n",
       "      <td>11.096865</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29453</th>\n",
       "      <td>bc0aa97b3aa53298a3eac9f82152b266e3359154</td>\n",
       "      <td>600</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>6.65</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Dispatch Taxi Affiliation</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>41.877406</td>\n",
       "      <td>-87.621972</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>279.229500</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>257.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>45</td>\n",
       "      <td>0.785416</td>\n",
       "      <td>19.983967</td>\n",
       "      <td>10.561540</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        trip_id  trip_seconds  \\\n",
       "52461  40711a363473d44fdfeed72464024e35c46cd180           240   \n",
       "20175  83b144b4dd2674bb052e9a8bafb9ab5198897392          1380   \n",
       "30072  1992605e75f6a48a08fe96788d6751a025375e4d           420   \n",
       "50956  33521c729d3c319dbd7d3cc7666210c46c39df59           900   \n",
       "29453  bc0aa97b3aa53298a3eac9f82152b266e3359154           600   \n",
       "\n",
       "       pickup_community_area  dropoff_community_area   fare payment_type  \\\n",
       "52461                     32                       8   5.00         Cash   \n",
       "20175                     41                      44  16.65         Cash   \n",
       "30072                      8                       8   6.45         Cash   \n",
       "50956                      7                       8  10.75  Credit Card   \n",
       "29453                     32                      32   6.65         Cash   \n",
       "\n",
       "                         company  pickup_centroid_latitude  \\\n",
       "52461  Dispatch Taxi Affiliation                 41.884987   \n",
       "20175  Taxi Affiliation Services                 41.794090   \n",
       "30072  Dispatch Taxi Affiliation                 41.899602   \n",
       "50956  Taxi Affiliation Services                 41.929263   \n",
       "29453  Dispatch Taxi Affiliation                 41.880994   \n",
       "\n",
       "       pickup_centroid_longitude  dropoff_centroid_latitude  \\\n",
       "52461                 -87.620993                  41.892508   \n",
       "20175                 -87.592311                  41.740206   \n",
       "30072                 -87.633308                  41.899602   \n",
       "50956                 -87.635891                  41.892042   \n",
       "29453                 -87.632746                  41.877406   \n",
       "\n",
       "       dropoff_centroid_longitude  humidity  pressure  temperature  \\\n",
       "52461                  -87.626215      64.0    1026.0   275.480000   \n",
       "20175                  -87.615970     100.0    1018.0   284.959000   \n",
       "30072                  -87.633308      83.0    1032.0   285.390333   \n",
       "50956                  -87.631864     100.0    1020.0   281.893000   \n",
       "29453                  -87.621972      75.0    1017.0   279.229500   \n",
       "\n",
       "      weather_description  wind_direction  wind_speed  year  month  day  \\\n",
       "52461        sky is clear           290.0         6.0  2016     12   22   \n",
       "20175        sky is clear           209.0         7.0  2014     10    7   \n",
       "30072       moderate rain           208.0         2.0  2015      5    4   \n",
       "50956          few clouds            33.0         2.0  2016     10   25   \n",
       "29453        sky is clear           257.0         6.0  2015      4   21   \n",
       "\n",
       "       week_day  hour  minute  distance_miles  distance_ord  distance_mdw  \\\n",
       "52461         5    17       0        0.632597     20.161236     11.373879   \n",
       "20175         3     8       0        4.064949     25.086632      9.961739   \n",
       "30072         2    22      15        0.000000     19.550617     11.375130   \n",
       "50956         3     9      15        2.585329     18.904905     11.096865   \n",
       "29453         3    12      45        0.785416     19.983967     10.561540   \n",
       "\n",
       "       taxi_id_ind  \n",
       "52461          502  \n",
       "20175           19  \n",
       "30072          350  \n",
       "50956          568  \n",
       "29453          216  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_trips.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trip_id                        object\n",
       "trip_seconds                    int64\n",
       "pickup_community_area           int64\n",
       "dropoff_community_area          int64\n",
       "fare                          float64\n",
       "payment_type                   object\n",
       "company                        object\n",
       "pickup_centroid_latitude      float64\n",
       "pickup_centroid_longitude     float64\n",
       "dropoff_centroid_latitude     float64\n",
       "dropoff_centroid_longitude    float64\n",
       "humidity                      float64\n",
       "pressure                      float64\n",
       "temperature                   float64\n",
       "weather_description            object\n",
       "wind_direction                float64\n",
       "wind_speed                    float64\n",
       "year                            int64\n",
       "month                           int64\n",
       "day                             int64\n",
       "week_day                        int64\n",
       "hour                            int64\n",
       "minute                          int64\n",
       "distance_miles                float64\n",
       "distance_ord                  float64\n",
       "distance_mdw                  float64\n",
       "taxi_id_ind                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_trips.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_trips = chicago_trips.drop(['trip_id','trip_seconds'],axis=1)\n",
    "chicago_trips = pd.get_dummies(chicago_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_train = chicago_trips[(chicago_trips['year']<2016) |\n",
    "                             ((chicago_trips['year']==2016) & (chicago_trips['month']<=6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47391, 161)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_test_val = chicago_trips[(chicago_trips['year']>2016) |\n",
    "                             ((chicago_trips['year']==2016) & (chicago_trips['month']>6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9531, 161)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_test_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_validation = chicago_test_val.sample(frac=0.5,random_state=13)\n",
    "chicago_test = chicago_test_val.sample(frac=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = chicago_train[['fare']]\n",
    "train_predictors = chicago_train.drop(['fare'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = chicago_validation[['fare']]\n",
    "validation_predictors = chicago_validation.drop(['fare'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del chicago_trips,chicago_train,chicago_test_val,chicago_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(target,predictors):\n",
    "    return sqrt(mean_squared_error(target, predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAL REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the linear regression we get a rmse of 8981371.08018 $\n",
      "CPU times: user 547 ms, sys: 109 ms, total: 655 ms\n",
      "Wall time: 206 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi_model_lin_reg = LinearRegression()\n",
    "taxi_model_lin_reg.fit(train_predictors, train_target)\n",
    "\n",
    "train_predictions = taxi_model.predict(train_predictors)\n",
    "val_predictions = taxi_model_lin_reg.predict(validation_predictors)\n",
    "\n",
    "error_train = rmse(train_target, train_predictions)\n",
    "error_val = rmse(validation_target, val_predictions)\n",
    "difference = error_val - error_train\n",
    "print ('The RMSE_train is %.5f $, the RMSE_val is %.5f $ and the difference %.5f $'\n",
    "               % (error_train,error_val,difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dicc_dt={}\n",
    "for depth in range(1,20):\n",
    "    taxi_model = DecisionTreeRegressor(max_depth=depth,\n",
    "                                       random_state=13)\n",
    "    taxi_model.fit(train_predictors, train_target)\n",
    "    train_predictions = taxi_model.predict(train_predictors)\n",
    "    val_predictions = taxi_model.predict(validation_predictors)\n",
    "    error_train = rmse(train_target, train_predictions)\n",
    "    error_val = rmse(validation_target, val_predictions)\n",
    "    difference = error_val - error_train\n",
    "    dicc_dt[str(depth)]= [error_train,error_val,difference]\n",
    "    print ('For a %d depth the RMSE_train is %.5f $, the RMSE_val is %.5f $ and the difference %.5f $'\n",
    "               % (depth,error_train,error_val,difference)) \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dicc_dt.items(), key=lambda x: x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dicc_dt.items(), key=lambda x: x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dicc_dt.items(), key=lambda x: x[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FORREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dicc_rf={}\n",
    "for depth in range(2,11):\n",
    "    print('Depth: %d' % (depth))\n",
    "    for estimator in range (50,151,5):\n",
    "        for leaves in [50,100,200,500,1000,5000]:\n",
    "            #print('Estimator: %d' % (estimator))\n",
    "            taxi_model_rf = RandomForestRegressor(n_estimators=estimator,\n",
    "                                                  max_depth=depth,\n",
    "                                                  max_leaf_nodes=leaves,\n",
    "                                                  n_jobs=-1)\n",
    "            taxi_model_rf.fit(train_predictors, train_target)                                    \n",
    "            train_predictions = taxi_model.predict(train_predictors)\n",
    "            val_predictions = taxi_model.predict(validation_predictors)\n",
    "            error_train = rmse(train_target, train_predictions)\n",
    "            error_val = rmse(validation_target, val_predictions)\n",
    "            difference = error_val - error_train\n",
    "            dicc_rf[str(depth) + '_' + str(estimator)+'_'+str(leaves)]= [error_train,error_val,difference]\n",
    "            print ('For a %d estimators and %d leaves the RMSE_train is %.5f $, the RMSE_val is %.5f $ and the difference %.5f $'\n",
    "               % (estimator,leaves,error_train,error_val,difference))  \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dicc_rf.items(), key=lambda x: x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dicc_rf.items(), key=lambda x: x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dicc_rf.items(), key=lambda x: x[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 2\n",
      "For a 50 estimators the RMSE_train is 2.46436 $ and the RMSE_val is 2.98450 $ and the difference 0.52014 $\n",
      "For a 55 estimators the RMSE_train is 2.44765 $ and the RMSE_val is 2.94054 $ and the difference 0.49290 $\n",
      "For a 60 estimators the RMSE_train is 2.42973 $ and the RMSE_val is 2.90460 $ and the difference 0.47487 $\n",
      "For a 65 estimators the RMSE_train is 2.41635 $ and the RMSE_val is 2.87549 $ and the difference 0.45914 $\n",
      "For a 70 estimators the RMSE_train is 2.39789 $ and the RMSE_val is 2.85295 $ and the difference 0.45506 $\n",
      "For a 75 estimators the RMSE_train is 2.38294 $ and the RMSE_val is 2.83252 $ and the difference 0.44958 $\n",
      "For a 80 estimators the RMSE_train is 2.37209 $ and the RMSE_val is 2.81455 $ and the difference 0.44246 $\n",
      "For a 85 estimators the RMSE_train is 2.35898 $ and the RMSE_val is 2.79668 $ and the difference 0.43770 $\n",
      "For a 90 estimators the RMSE_train is 2.35005 $ and the RMSE_val is 2.78544 $ and the difference 0.43539 $\n",
      "For a 95 estimators the RMSE_train is 2.34517 $ and the RMSE_val is 2.77059 $ and the difference 0.42542 $\n",
      "For a 100 estimators the RMSE_train is 2.33918 $ and the RMSE_val is 2.76317 $ and the difference 0.42399 $\n",
      "For a 105 estimators the RMSE_train is 2.32673 $ and the RMSE_val is 2.75096 $ and the difference 0.42423 $\n",
      "For a 110 estimators the RMSE_train is 2.31771 $ and the RMSE_val is 2.74302 $ and the difference 0.42531 $\n",
      "For a 115 estimators the RMSE_train is 2.31312 $ and the RMSE_val is 2.73959 $ and the difference 0.42647 $\n",
      "For a 120 estimators the RMSE_train is 2.30239 $ and the RMSE_val is 2.73256 $ and the difference 0.43017 $\n",
      "For a 125 estimators the RMSE_train is 2.29829 $ and the RMSE_val is 2.72489 $ and the difference 0.42660 $\n",
      "For a 130 estimators the RMSE_train is 2.29094 $ and the RMSE_val is 2.71785 $ and the difference 0.42692 $\n",
      "For a 135 estimators the RMSE_train is 2.28542 $ and the RMSE_val is 2.71471 $ and the difference 0.42930 $\n",
      "For a 140 estimators the RMSE_train is 2.28030 $ and the RMSE_val is 2.70884 $ and the difference 0.42854 $\n",
      "For a 145 estimators the RMSE_train is 2.27482 $ and the RMSE_val is 2.70207 $ and the difference 0.42725 $\n",
      "For a 150 estimators the RMSE_train is 2.27142 $ and the RMSE_val is 2.69732 $ and the difference 0.42589 $\n",
      "For a 155 estimators the RMSE_train is 2.26804 $ and the RMSE_val is 2.69299 $ and the difference 0.42495 $\n",
      "For a 160 estimators the RMSE_train is 2.26338 $ and the RMSE_val is 2.69061 $ and the difference 0.42723 $\n",
      "For a 165 estimators the RMSE_train is 2.25906 $ and the RMSE_val is 2.68844 $ and the difference 0.42938 $\n",
      "For a 170 estimators the RMSE_train is 2.25544 $ and the RMSE_val is 2.68599 $ and the difference 0.43055 $\n",
      "For a 175 estimators the RMSE_train is 2.25278 $ and the RMSE_val is 2.68423 $ and the difference 0.43146 $\n",
      "For a 180 estimators the RMSE_train is 2.24988 $ and the RMSE_val is 2.68168 $ and the difference 0.43179 $\n",
      "For a 185 estimators the RMSE_train is 2.24714 $ and the RMSE_val is 2.67608 $ and the difference 0.42894 $\n",
      "For a 190 estimators the RMSE_train is 2.24449 $ and the RMSE_val is 2.67467 $ and the difference 0.43018 $\n",
      "For a 195 estimators the RMSE_train is 2.24095 $ and the RMSE_val is 2.67221 $ and the difference 0.43126 $\n",
      "For a 200 estimators the RMSE_train is 2.23879 $ and the RMSE_val is 2.66715 $ and the difference 0.42836 $\n",
      "Depth: 3\n",
      "For a 50 estimators the RMSE_train is 2.31059 $ and the RMSE_val is 2.76293 $ and the difference 0.45234 $\n",
      "For a 55 estimators the RMSE_train is 2.29551 $ and the RMSE_val is 2.72108 $ and the difference 0.42557 $\n",
      "For a 60 estimators the RMSE_train is 2.27930 $ and the RMSE_val is 2.69028 $ and the difference 0.41098 $\n",
      "For a 65 estimators the RMSE_train is 2.26709 $ and the RMSE_val is 2.66736 $ and the difference 0.40027 $\n",
      "For a 70 estimators the RMSE_train is 2.25579 $ and the RMSE_val is 2.65867 $ and the difference 0.40288 $\n",
      "For a 75 estimators the RMSE_train is 2.24319 $ and the RMSE_val is 2.64715 $ and the difference 0.40396 $\n",
      "For a 80 estimators the RMSE_train is 2.23360 $ and the RMSE_val is 2.64499 $ and the difference 0.41139 $\n",
      "For a 85 estimators the RMSE_train is 2.22494 $ and the RMSE_val is 2.63972 $ and the difference 0.41479 $\n",
      "For a 90 estimators the RMSE_train is 2.21461 $ and the RMSE_val is 2.63431 $ and the difference 0.41970 $\n",
      "For a 95 estimators the RMSE_train is 2.20616 $ and the RMSE_val is 2.62726 $ and the difference 0.42111 $\n",
      "For a 100 estimators the RMSE_train is 2.19908 $ and the RMSE_val is 2.62560 $ and the difference 0.42651 $\n",
      "For a 105 estimators the RMSE_train is 2.19268 $ and the RMSE_val is 2.61863 $ and the difference 0.42595 $\n",
      "For a 110 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 115 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 120 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 125 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 130 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 135 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 140 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 145 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 150 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 155 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 160 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 165 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 170 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 175 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 180 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 185 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 190 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 195 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "For a 200 estimators the RMSE_train is 2.18951 $ and the RMSE_val is 2.61682 $ and the difference 0.42732 $\n",
      "Depth: 4\n",
      "For a 50 estimators the RMSE_train is 2.20889 $ and the RMSE_val is 2.71552 $ and the difference 0.50663 $\n",
      "For a 55 estimators the RMSE_train is 2.19564 $ and the RMSE_val is 2.69582 $ and the difference 0.50018 $\n",
      "For a 60 estimators the RMSE_train is 2.17800 $ and the RMSE_val is 2.68177 $ and the difference 0.50377 $\n",
      "For a 65 estimators the RMSE_train is 2.16236 $ and the RMSE_val is 2.67370 $ and the difference 0.51134 $\n",
      "For a 70 estimators the RMSE_train is 2.15024 $ and the RMSE_val is 2.66399 $ and the difference 0.51374 $\n",
      "For a 75 estimators the RMSE_train is 2.14187 $ and the RMSE_val is 2.65871 $ and the difference 0.51684 $\n",
      "For a 80 estimators the RMSE_train is 2.12860 $ and the RMSE_val is 2.65442 $ and the difference 0.52582 $\n",
      "For a 85 estimators the RMSE_train is 2.12112 $ and the RMSE_val is 2.65102 $ and the difference 0.52990 $\n",
      "For a 90 estimators the RMSE_train is 2.11477 $ and the RMSE_val is 2.64483 $ and the difference 0.53006 $\n",
      "For a 95 estimators the RMSE_train is 2.10553 $ and the RMSE_val is 2.63876 $ and the difference 0.53323 $\n",
      "For a 100 estimators the RMSE_train is 2.09921 $ and the RMSE_val is 2.63608 $ and the difference 0.53686 $\n",
      "For a 105 estimators the RMSE_train is 2.09041 $ and the RMSE_val is 2.63197 $ and the difference 0.54157 $\n",
      "For a 110 estimators the RMSE_train is 2.08449 $ and the RMSE_val is 2.63026 $ and the difference 0.54576 $\n",
      "For a 115 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a 120 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 125 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 130 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 135 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 140 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 145 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 150 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 155 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 160 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 165 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 170 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 175 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 180 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 185 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 190 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 195 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "For a 200 estimators the RMSE_train is 2.08315 $ and the RMSE_val is 2.62962 $ and the difference 0.54647 $\n",
      "Depth: 5\n",
      "For a 50 estimators the RMSE_train is 2.10722 $ and the RMSE_val is 2.62696 $ and the difference 0.51974 $\n",
      "For a 55 estimators the RMSE_train is 2.09218 $ and the RMSE_val is 2.61649 $ and the difference 0.52431 $\n",
      "For a 60 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 65 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 70 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 75 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 80 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 85 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 90 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 95 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 100 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 105 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 110 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 115 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 120 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 125 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 130 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 135 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 140 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 145 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 150 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 155 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 160 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 165 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 170 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 175 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 180 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 185 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 190 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 195 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "For a 200 estimators the RMSE_train is 2.08490 $ and the RMSE_val is 2.61036 $ and the difference 0.52547 $\n",
      "Depth: 6\n",
      "For a 50 estimators the RMSE_train is 2.00814 $ and the RMSE_val is 2.58137 $ and the difference 0.57324 $\n",
      "For a 55 estimators the RMSE_train is 1.98406 $ and the RMSE_val is 2.56765 $ and the difference 0.58359 $\n",
      "For a 60 estimators the RMSE_train is 1.96512 $ and the RMSE_val is 2.55960 $ and the difference 0.59448 $\n",
      "For a 65 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 70 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 75 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 80 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 85 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 90 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 95 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 100 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 105 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 110 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 115 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 120 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 125 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 130 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 135 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 140 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 145 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 150 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 155 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 160 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 165 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 170 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 175 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 180 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 185 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a 190 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 195 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "For a 200 estimators the RMSE_train is 1.95584 $ and the RMSE_val is 2.55785 $ and the difference 0.60201 $\n",
      "Depth: 7\n",
      "For a 50 estimators the RMSE_train is 1.89370 $ and the RMSE_val is 2.59708 $ and the difference 0.70338 $\n",
      "For a 55 estimators the RMSE_train is 1.86889 $ and the RMSE_val is 2.59080 $ and the difference 0.72191 $\n",
      "For a 60 estimators the RMSE_train is 1.84788 $ and the RMSE_val is 2.58643 $ and the difference 0.73855 $\n",
      "For a 65 estimators the RMSE_train is 1.83037 $ and the RMSE_val is 2.58462 $ and the difference 0.75424 $\n",
      "For a 70 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 75 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 80 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 85 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 90 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 95 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 100 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 105 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 110 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 115 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 120 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 125 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 130 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 135 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 140 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 145 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 150 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 155 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 160 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 165 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 170 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 175 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 180 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 185 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 190 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 195 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "For a 200 estimators the RMSE_train is 1.82271 $ and the RMSE_val is 2.58397 $ and the difference 0.76126 $\n",
      "Depth: 8\n",
      "For a 50 estimators the RMSE_train is 1.76317 $ and the RMSE_val is 2.67588 $ and the difference 0.91271 $\n",
      "For a 55 estimators the RMSE_train is 1.73172 $ and the RMSE_val is 2.67075 $ and the difference 0.93903 $\n",
      "For a 60 estimators the RMSE_train is 1.71275 $ and the RMSE_val is 2.66523 $ and the difference 0.95248 $\n",
      "For a 65 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 70 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 75 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 80 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 85 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 90 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 95 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 100 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 105 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 110 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 115 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 120 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 125 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 130 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 135 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 140 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 145 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 150 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 155 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 160 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 165 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 170 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 175 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 180 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 185 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 190 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 195 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "For a 200 estimators the RMSE_train is 1.68083 $ and the RMSE_val is 2.65768 $ and the difference 0.97685 $\n",
      "Depth: 9\n",
      "For a 50 estimators the RMSE_train is 1.63625 $ and the RMSE_val is 2.74931 $ and the difference 1.11306 $\n",
      "For a 55 estimators the RMSE_train is 1.60492 $ and the RMSE_val is 2.74502 $ and the difference 1.14010 $\n",
      "For a 60 estimators the RMSE_train is 1.57228 $ and the RMSE_val is 2.74050 $ and the difference 1.16822 $\n",
      "For a 65 estimators the RMSE_train is 1.54612 $ and the RMSE_val is 2.73476 $ and the difference 1.18864 $\n",
      "For a 70 estimators the RMSE_train is 1.52335 $ and the RMSE_val is 2.73409 $ and the difference 1.21074 $\n",
      "For a 75 estimators the RMSE_train is 1.51350 $ and the RMSE_val is 2.73401 $ and the difference 1.22051 $\n",
      "For a 80 estimators the RMSE_train is 1.48015 $ and the RMSE_val is 2.73326 $ and the difference 1.25310 $\n",
      "For a 85 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 90 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 95 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 100 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a 105 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 110 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 115 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 120 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 125 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 130 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 135 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 140 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 145 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 150 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 155 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 160 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 165 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 170 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 175 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 180 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 185 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 190 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 195 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "For a 200 estimators the RMSE_train is 1.47092 $ and the RMSE_val is 2.73317 $ and the difference 1.26225 $\n",
      "Depth: 10\n",
      "For a 50 estimators the RMSE_train is 1.49576 $ and the RMSE_val is 2.75699 $ and the difference 1.26123 $\n",
      "For a 55 estimators the RMSE_train is 1.45555 $ and the RMSE_val is 2.75110 $ and the difference 1.29555 $\n",
      "For a 60 estimators the RMSE_train is 1.42134 $ and the RMSE_val is 2.74701 $ and the difference 1.32566 $\n",
      "For a 65 estimators the RMSE_train is 1.39034 $ and the RMSE_val is 2.74556 $ and the difference 1.35521 $\n",
      "For a 70 estimators the RMSE_train is 1.35659 $ and the RMSE_val is 2.74454 $ and the difference 1.38795 $\n",
      "For a 75 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 80 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 85 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 90 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 95 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 100 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 105 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 110 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 115 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 120 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 125 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 130 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 135 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 140 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 145 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 150 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 155 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 160 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 165 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 170 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 175 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 180 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 185 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 190 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 195 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "For a 200 estimators the RMSE_train is 1.33523 $ and the RMSE_val is 2.74208 $ and the difference 1.40685 $\n",
      "CPU times: user 3h 12min 22s, sys: 48.6 s, total: 3h 13min 10s\n",
      "Wall time: 3h 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dicc_xgb={}\n",
    "for depth in range(2,11):\n",
    "    print('Depth: %d' % (depth))\n",
    "    for estimator in range (50,201,5):\n",
    "        taxi_model = XGBRegressor(max_depth=depth,\n",
    "                                  learning_rate=0.1,\n",
    "                                  n_estimators=estimator,\n",
    "                                  n_jobs=-1)\n",
    "        taxi_model.fit(train_predictors,\n",
    "                       train_target,\n",
    "                       early_stopping_rounds=5,\n",
    "                       eval_set=[(validation_predictors, validation_target)],\n",
    "                       verbose=False)\n",
    "        train_predictions = taxi_model.predict(train_predictors)\n",
    "        val_predictions = taxi_model.predict(validation_predictors)\n",
    "        error_train = rmse(train_target, train_predictions)\n",
    "        error_val = rmse(validation_target, val_predictions)\n",
    "        difference = error_val - error_train\n",
    "        dicc_xgb[str(depth) + '_' + str(estimator)]= [error_train,error_val,difference]\n",
    "        print ('For a %d estimators the RMSE_train is %.5f $, the RMSE_val is %.5f $ and the difference %.5f $'\n",
    "               % (estimator,error_train,error_val,difference))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, [1.3352252905353257, 2.7420787416374357, 1.40685345110211])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(dicc_xgb.items(), key=lambda x: x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, [1.9558425752015027, 2.5578522931487924, 0.6020097179472896])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(dicc_xgb.items(), key=lambda x: x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, [2.3105932651669367, 2.7629330904040867, 0.45233982523715])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(dicc_xgb.items(), key=lambda x: x[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_train</th>\n",
       "      <th>error_val</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2.464360</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>0.520140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.893700</td>\n",
       "      <td>2.597082</td>\n",
       "      <td>0.703381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.868889</td>\n",
       "      <td>2.590800</td>\n",
       "      <td>0.721911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.847884</td>\n",
       "      <td>2.586430</td>\n",
       "      <td>0.738547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.830375</td>\n",
       "      <td>2.584615</td>\n",
       "      <td>0.754240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    error_train  error_val  difference\n",
       "52     2.464360   2.984500    0.520140\n",
       "57     1.893700   2.597082    0.703381\n",
       "62     1.868889   2.590800    0.721911\n",
       "67     1.847884   2.586430    0.738547\n",
       "72     1.830375   2.584615    0.754240"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_df = pd.DataFrame.from_dict(dicc_xgb1).T\n",
    "xgb_df.columns=['error_train','error_val','difference']\n",
    "xgb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-2bbaa8f3bbda>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-2bbaa8f3bbda>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ax.set_title('RMSE $', fontsize=40)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ax=xgb_df[xgb_df['error_train'].plot(kind='line', style='D-',figsize=(30, 15),fontsize=12, rot=0)\n",
    "ax.set_title('RMSE $', fontsize=40)\n",
    "ax.set_xlabel(\"Depth_Estimators\", fontsize=20)\n",
    "ax.set_ylabel(\"Rmse $\", fontsize=20)\n",
    "\n",
    "xgb_df['error_val'].plot(kind='line',style='.-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}, \"mark\": {\"tooltip\": null}}, \"data\": {\"name\": \"data-cfc1cf8d85817d5d4d027b668cbe4c91\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"difference\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"error_train\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.2.1.json\", \"datasets\": {\"data-cfc1cf8d85817d5d4d027b668cbe4c91\": [{\"index\": 52, \"error_train\": 2.464359869764304, \"error_val\": 2.9844999732847213, \"difference\": 0.5201401035204172}, {\"index\": 57, \"error_train\": 1.8937002727036445, \"error_val\": 2.597081621915817, \"difference\": 0.7033813492121723}, {\"index\": 62, \"error_train\": 1.8688886134784402, \"error_val\": 2.5907996248756286, \"difference\": 0.7219110113971885}, {\"index\": 67, \"error_train\": 1.8478835359335006, \"error_val\": 2.586430368603242, \"difference\": 0.7385468326697415}, {\"index\": 72, \"error_train\": 1.8303747823620087, \"error_val\": 2.584615104088738, \"difference\": 0.7542403217267293}, {\"index\": 77, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 82, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 87, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 92, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 97, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 102, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 107, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 112, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 117, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 122, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 127, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 132, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 137, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 142, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 147, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 152, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 157, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 162, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 167, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 172, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 177, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 182, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 187, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 192, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 197, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 202, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 53, \"error_train\": 2.3105932651669367, \"error_val\": 2.7629330904040867, \"difference\": 0.45233982523715}, {\"index\": 58, \"error_train\": 1.7631700119740923, \"error_val\": 2.675882768817707, \"difference\": 0.9127127568436146}, {\"index\": 63, \"error_train\": 1.731724318975934, \"error_val\": 2.6707544466637647, \"difference\": 0.9390301276878308}, {\"index\": 68, \"error_train\": 1.7127513417161941, \"error_val\": 2.6652301575148196, \"difference\": 0.9524788157986255}, {\"index\": 73, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 78, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 83, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 88, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 93, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 98, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 103, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 108, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 113, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 118, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 123, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 128, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 133, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 138, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 143, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 148, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 153, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 158, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 163, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 168, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 173, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 178, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 183, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 188, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 193, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 198, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 203, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 54, \"error_train\": 2.2088902411852582, \"error_val\": 2.715522187952455, \"difference\": 0.5066319467671967}, {\"index\": 59, \"error_train\": 1.6362531475433815, \"error_val\": 2.749310527836128, \"difference\": 1.1130573802927466}, {\"index\": 64, \"error_train\": 1.6049170609903465, \"error_val\": 2.7450198199444578, \"difference\": 1.1401027589541113}, {\"index\": 69, \"error_train\": 1.5722792370629466, \"error_val\": 2.740500855576563, \"difference\": 1.1682216185136163}, {\"index\": 74, \"error_train\": 1.5461216530003326, \"error_val\": 2.734756949410122, \"difference\": 1.1886352964097895}, {\"index\": 79, \"error_train\": 1.523350848264002, \"error_val\": 2.7340921516542194, \"difference\": 1.2107413033902175}, {\"index\": 84, \"error_train\": 1.5135025891992384, \"error_val\": 2.734013401779099, \"difference\": 1.2205108125798605}, {\"index\": 89, \"error_train\": 1.4801520704489313, \"error_val\": 2.733255935095036, \"difference\": 1.2531038646461046}, {\"index\": 94, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 99, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 104, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 109, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 114, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 119, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 124, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 129, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 134, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 139, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 144, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 149, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 154, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 159, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 164, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 169, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 174, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 179, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 184, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 189, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 194, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 199, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 204, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 55, \"error_train\": 2.1072162735445867, \"error_val\": 2.6269612292636615, \"difference\": 0.5197449557190748}, {\"index\": 60, \"error_train\": 1.4957589500935702, \"error_val\": 2.7569894800249624, \"difference\": 1.2612305299313922}, {\"index\": 65, \"error_train\": 1.4555495150306699, \"error_val\": 2.751103174875137, \"difference\": 1.2955536598444672}, {\"index\": 70, \"error_train\": 1.4213431921271857, \"error_val\": 2.7470064901861275, \"difference\": 1.3256632980589418}, {\"index\": 75, \"error_train\": 1.390344380934316, \"error_val\": 2.745558305662036, \"difference\": 1.3552139247277202}, {\"index\": 80, \"error_train\": 1.3565878673530616, \"error_val\": 2.7445367893348, \"difference\": 1.3879489219817382}, {\"index\": 85, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 90, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 95, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 100, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 105, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 110, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 115, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 120, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 125, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 130, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 135, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 140, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 145, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 150, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 155, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 160, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 165, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 170, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 175, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 180, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 185, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 190, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 195, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 200, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 205, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}, {\"index\": 56, \"error_train\": 2.0081393934017546, \"error_val\": 2.5813747633774478, \"difference\": 0.5732353699756931}, {\"index\": 61, \"error_train\": 1.9840631184436115, \"error_val\": 2.567651562012912, \"difference\": 0.5835884435693004}, {\"index\": 66, \"error_train\": 1.965124020222579, \"error_val\": 2.5596047814247367, \"difference\": 0.5944807612021576}, {\"index\": 71, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 76, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 81, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 86, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 91, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 96, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 101, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 106, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 111, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 116, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 121, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 126, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 131, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 136, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 141, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 146, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 151, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 156, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 161, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 166, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 171, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 176, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 181, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 186, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 191, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 196, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 201, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 206, \"error_train\": 1.9558425752015027, \"error_val\": 2.5578522931487924, \"difference\": 0.6020097179472896}, {\"index\": 207, \"error_train\": 1.8227057456561022, \"error_val\": 2.5839671644275266, \"difference\": 0.7612614187714244}, {\"index\": 208, \"error_train\": 1.6808343160131176, \"error_val\": 2.6576821478540364, \"difference\": 0.9768478318409188}, {\"index\": 209, \"error_train\": 1.47091977583846, \"error_val\": 2.733166215340769, \"difference\": 1.262246439502309}, {\"index\": 210, \"error_train\": 1.3352252905353257, \"error_val\": 2.7420787416374357, \"difference\": 1.40685345110211}]}};\n",
       "var opt = {};\n",
       "var type = \"vega-lite\";\n",
       "var id = \"a9948769-669e-4e45-9e7d-d03e1f5ffc7d\";\n",
       "\n",
       "var output_area = this;\n",
       "\n",
       "require([\"nbextensions/jupyter-vega/index\"], function(vega) {\n",
       "  var target = document.createElement(\"div\");\n",
       "  target.id = id;\n",
       "  target.className = \"vega-embed\";\n",
       "\n",
       "  var style = document.createElement(\"style\");\n",
       "  style.textContent = [\n",
       "    \".vega-embed .error p {\",\n",
       "    \"  color: firebrick;\",\n",
       "    \"  font-size: 14px;\",\n",
       "    \"}\",\n",
       "  ].join(\"\\\\n\");\n",
       "\n",
       "  // element is a jQuery wrapped DOM element inside the output area\n",
       "  // see http://ipython.readthedocs.io/en/stable/api/generated/\\\n",
       "  // IPython.display.html#IPython.display.Javascript.__init__\n",
       "  element[0].appendChild(target);\n",
       "  element[0].appendChild(style);\n",
       "\n",
       "  vega.render(\"#\" + id, spec, type, opt, output_area);\n",
       "}, function (err) {\n",
       "  if (err.requireType !== \"scripterror\") {\n",
       "    throw(err);\n",
       "  }\n",
       "});\n"
      ],
      "text/plain": [
       "<vega.vegalite.VegaLite at 0x12136d8d0>"
      ]
     },
     "metadata": {
      "jupyter-vega": "#a9948769-669e-4e45-9e7d-d03e1f5ffc7d"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "jupyter-vega": "#a9948769-669e-4e45-9e7d-d03e1f5ffc7d"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alt.Chart(xgb_df.reset_index()).mark_line().encode(x ='index',\n",
    "                                     y=\"error_train\",\n",
    "                                    color='difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
