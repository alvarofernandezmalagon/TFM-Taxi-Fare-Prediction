{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling all the Data\n",
    "\n",
    "After the analysis performed with `taxi_model_sample001.csv` in the notebook `model_sample.ipynb` to see which model is best suited and its hyperparameters, we will perform the modeling of `taxi_model.csv` and check which is its RMSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago Coordinates \n",
    "-87.6244212, 41.8755616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the libraries we are going to use in our analysis\n",
    "\n",
    "# Libraries to manage our dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Libraries for modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# A library that will allow us to measure how good our model is.\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Bagging + Xgboost with depth 5 and 120 estimators full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure below how good our model is by predicting the rates of the test data that we separate at the beginning of the notebook but this time with all the data not with the sample created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to read the data at 2019-05-15 21:35:18.208675\n",
      "Finished read the data at 2019-05-15 21:35:21.061187\n",
      "Starting to split the data in train, validation and test at 2019-05-15 21:35:21.061297\n",
      "The train dataset shape is: 0,22\n",
      "The valitation/test dataset shape is: 0,22\n",
      "The train predictors shape is 0,21 and the train target shape is 0\n",
      "The validation predictors shape is 0,21 and the validation target shape is 0\n",
      "The test predictors shape is 0,21 and the test target shape is 0\n",
      "Finished split the data in train, validation and test at 2019-05-15 21:35:21.556523\n",
      "Starting to train at 2019-05-15 21:35:21.556681\n",
      "Finished train at 2019-05-15 21:35:21.556736\n",
      "Starting to fit at 2019-05-15 21:35:21.556759\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 21)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    279\u001b[0m         X, y = check_X_y(\n\u001b[1;32m    280\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         )\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 582\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 21)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Starting to read the data at %s' %(datetime.now()))\n",
    "chicago_trips = pd.read_csv('../Data/taxi_model.csv')\n",
    "chicago_trips = chicago_trips.drop(['trip_id','trip_seconds','taxi_id_ind','payment_type_ind'],axis=1)\n",
    "print('Finished read the data at %s' %(datetime.now()))\n",
    "\n",
    "print('Starting to split the data in train, validation and test at %s' %(datetime.now()))\n",
    "chicago_trips = pd.get_dummies(chicago_trips)\n",
    "chicago_train = chicago_trips[(chicago_trips['year']<2016) |\n",
    "                             ((chicago_trips['year']==2016) & (chicago_trips['month']<=1))]\n",
    "\n",
    "chicago_test_val = chicago_trips[(chicago_trips['year']==2017) |\n",
    "                             ((chicago_trips['year']==2016) & (chicago_trips['month']>1))]\n",
    "\n",
    "print('The train dataset shape is: %d,%d' %(chicago_train.shape[0],chicago_train.shape[1]))\n",
    "print('The valitation/test dataset shape is: %d,%d' %(chicago_test_val.shape[0],chicago_test_val.shape[1]))\n",
    "\n",
    "\n",
    "train_target = np.ravel(chicago_train[['fare']])\n",
    "train_predictors = chicago_train.drop(['fare'],axis=1)\n",
    "\n",
    "chicago_test_val_target = np.ravel(chicago_test_val[['fare']])\n",
    "chicago_test_val_predictors = chicago_test_val.drop(['fare'],axis=1)\n",
    "\n",
    "validation_predictors,test_predictors, validation_target, test_target = train_test_split(chicago_test_val_predictors, \n",
    "                                                                                         chicago_test_val_target, \n",
    "                                                                                         test_size=0.38,\n",
    "                                                                                         random_state=42)\n",
    "\n",
    "print('The train predictors shape is %d,%d and the train target shape is %d' %(train_predictors.shape[0],\n",
    "                                                                                  train_predictors.shape[1],\n",
    "                                                                                  train_target.shape[0]))\n",
    "\n",
    "print('The validation predictors shape is %d,%d and the validation target shape is %d' \n",
    "      %(validation_predictors.shape[0],validation_predictors.shape[1],\n",
    "        validation_target.shape[0]))\n",
    "\n",
    "print('The test predictors shape is %d,%d and the test target shape is %d' %(test_predictors.shape[0],\n",
    "                                                                          test_predictors.shape[1],\n",
    "                                                                          test_target.shape[0]))\n",
    "\n",
    "del chicago_test_val, chicago_test_val_predictors, chicago_test_val_target, chicago_train, chicago_trips\n",
    "\n",
    "del validation_predictors, validation_target\n",
    "print('Finished split the data in train, validation and test at %s' %(datetime.now()))\n",
    "\n",
    "depth = 5\n",
    "estimator = 120\n",
    "\n",
    "print('Starting to train at %s' %(datetime.now()))\n",
    "taxi_model_bg_xgb = BaggingRegressor(XGBRegressor(max_depth=depth,\n",
    "                                                  learning_rate=0.1,\n",
    "                                                  n_estimators=estimator,\n",
    "                                                  n_jobs=-1,\n",
    "                                                  seed=13),\n",
    "                                     n_jobs= -1,\n",
    "                                     random_state=13)\n",
    "print('Finished train at %s' %(datetime.now()))\n",
    "\n",
    "print('Starting to fit at %s' %(datetime.now()))\n",
    "taxi_model_bg_xgb.fit(train_predictors, train_target)\n",
    "print('Finished fit at %s' %(datetime.now()))\n",
    "\n",
    "print('Starting to predict train at %s' %(datetime.now()))\n",
    "train_predictions = taxi_model_bg_xgb.predict(train_predictors)\n",
    "val_predictions = taxi_model_bg_xgb.predict(validation_predictors)\n",
    "test_predictions = taxi_model_bg_xgb.predict(test_predictors)\n",
    "print('Finished predict at %s' %(datetime.now()))\n",
    "\n",
    "error_train = rmse(train_target, train_predictions)\n",
    "error_val = rmse(validation_target, val_predictions)\n",
    "error_test = rmse(test_target, test_predictions)\n",
    "difference = error_val - error_train\n",
    "    \n",
    "\n",
    "print ('For a %d depth and %d estimators the RMSE_train is %.5f $, the RMSE_val is %.5f $ and the difference %.5f $'\n",
    "       % (depth,estimator, error_train,error_val,difference))\n",
    "\n",
    "print ('For a %d depth and %d estimators the RMSE_test is %.5f $'\n",
    "        % (depth, estimator, error_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
